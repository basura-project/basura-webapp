{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a0f124-b6ba-4ebe-8f0c-a09851dd3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import TimeLLM\n",
    "from TimeSeries import data_provider\n",
    "from tools import EarlyStopping, adjust_learning_rate, vali, load_content\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048763ac-2cef-4ffc-9fb4-360382b62f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e6dae9-f075-4a5b-9498-333cae28f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    root_path='./',  # Assuming the file is in the current directory\n",
    "    data_path='NVDA.csv',  # Your CSV file\n",
    "    seq_len=384,\n",
    "    label_len=96,\n",
    "    pred_len=96,\n",
    "    features='M',  # 'M' for multivariate, 'S' for univariate\n",
    "    target=None,  # Replace with the actual name of the target column in your CSV\n",
    "    embed='timeF',  # Use 'timeF' for time feature encoding\n",
    "    scale=True,\n",
    "    percent=100,\n",
    "    num_workers=0,\n",
    "    batch_size=2,\n",
    "    freq='d',\n",
    "    model_id='test',\n",
    "    model_comment='none',\n",
    "    model='TimeLLM',\n",
    "    seed=2021,\n",
    "    checkpoints='./checkpoints/',\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    e_layers=2,\n",
    "    d_layers=1,\n",
    "    d_ff=4096,\n",
    "    dropout=0.1,\n",
    "    activation='gelu',\n",
    "    eval_batch_size=1,\n",
    "    patience=10,\n",
    "    learning_rate=0.0001,\n",
    "    train_epochs=1,\n",
    "    loss='MSE',\n",
    "    lradj='type1',\n",
    "    use_amp=False,\n",
    "    llm_layers=12,\n",
    "    task_name = 'long_term_forecast',\n",
    "    llm_model = 'LLAMA',\n",
    "    llm_dim = 4096,\n",
    "    enc_in = 7,\n",
    "    patch_len = 16 ,\n",
    "    stride = 8,\n",
    "    prompt_domain = False,\n",
    "    content = None\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e524cd-fea5-4ac2-85cd-f5c8abf300b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1781bb361e4a1ba555f49cc2ef620b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model = TimeLLM.Model(args).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f0d736-c21d-4a49-a882-e0f87e541b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_x shape: (870, 6)\n",
      "data_y shape: (870, 6)\n",
      "data_x shape: (509, 6)\n",
      "data_y shape: (509, 6)\n",
      "data_x shape: (632, 6)\n",
      "data_y shape: (632, 6)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_loader = data_provider(args, 'train')\n",
    "vali_data, vali_loader = data_provider(args, 'val')\n",
    "test_data, test_loader = data_provider(args, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492f5ec7-9913-439a-a034-90049badfecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (llm_model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (patch_embedding): PatchEmbedding(\n",
       "    (padding_patch_layer): ReplicationPad1d()\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(16, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (mapping_layer): Linear(in_features=32000, out_features=1000, bias=True)\n",
       "  (reprogramming_layer): ReprogrammingLayer(\n",
       "    (query_projection): Linear(in_features=512, out_features=32768, bias=True)\n",
       "    (key_projection): Linear(in_features=4096, out_features=32768, bias=True)\n",
       "    (value_projection): Linear(in_features=4096, out_features=32768, bias=True)\n",
       "    (out_projection): Linear(in_features=32768, out_features=4096, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (output_projection): FlattenHead(\n",
       "    (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "    (linear): Linear(in_features=196608, out_features=96, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (normalize_layers): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98dcbb7d-db81-4584-b5df-5db9d684bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "103f03b6-01d4-4950-a906-1a651ffaeaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=args.learning_rate, epochs=args.train_epochs, steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee9cef43-0f3b-4135-9308-6310eefc28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.use_amp:\n",
    "    scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b691231-46b4-4c0f-9d5c-2ea79567285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=args.patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089a6ef-a017-4348-9316-d6b128cdb9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                  | 0/1173 [00:00<?, ?it/s]LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      " 34%|████████████████████████████████████████████████████████▎                                                                                                           | 403/1173 [4:52:31<9:30:00, 44.42s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.train_epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        batch_x, batch_y = batch_x.float().to(device), batch_y.float().to(device)\n",
    "        batch_x_mark, batch_y_mark = batch_x_mark.float().to(device), batch_y_mark.float().to(device)\n",
    "\n",
    "        # Decoder input\n",
    "        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).to(device)\n",
    "\n",
    "        if args.use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                loss = criterion(outputs, batch_y[:, -args.pred_len:, :])\n",
    "        else:\n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            loss = criterion(outputs, batch_y[:, -args.pred_len:, :])\n",
    "\n",
    "        if args.use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation and Early Stopping\n",
    "    train_loss = np.mean(train_loss)\n",
    "    vali_loss, _ = vali(args, model, vali_data, vali_loader, criterion)\n",
    "    early_stopping(vali_loss, model, path=args.checkpoints)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.6f}, Vali Loss: {vali_loss:.6f}, Time: {time.time()-epoch_start_time:.2f}s\")\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Testing phase\n",
    "model.load_state_dict(torch.load(args.checkpoints))\n",
    "test_loss, _ = vali(args, model, test_data, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d01fe6-5ee7-4eb2-a5e9-09d2014c3570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

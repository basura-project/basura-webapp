{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Qk4Uw_iSr3Mc",
      "metadata": {
        "id": "Qk4Uw_iSr3Mc"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### **Environment Setup:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "zVVh9clziIg3"
      },
      "id": "zVVh9clziIg3",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Necessary for Colab, not necessary for course environment\n",
        "%pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
        "%pip install -q arxiv pymupdf faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvNQvmVJheGY",
        "outputId": "fec777a4-c49a-465a-8b02-f95fb3464404"
      },
      "id": "rvNQvmVJheGY",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "for models in ChatNVIDIA.get_available_models():\n",
        "  print(models.id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_7l7AU_nur-",
        "outputId": "737ad9d2-cb6b-415a-a5f7-162dd49a3aa7"
      },
      "id": "2_7l7AU_nur-",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "google/deplot\n",
            "writer/palmyra-med-70b\n",
            "snowflake/arctic\n",
            "upstage/solar-10.7b-instruct\n",
            "ibm/granite-34b-code-instruct\n",
            "ibm/granite-8b-code-instruct\n",
            "writer/palmyra-fin-70b-32k\n",
            "google/gemma-2b\n",
            "mistralai/mistral-7b-instruct-v0.3\n",
            "nvidia/llama3-chatqa-1.5-70b\n",
            "01-ai/yi-large\n",
            "microsoft/phi-3-mini-4k-instruct\n",
            "deepseek-ai/deepseek-coder-6.7b-instruct\n",
            "liuhaotian/llava-v1.6-mistral-7b\n",
            "google/recurrentgemma-2b\n",
            "google/codegemma-1.1-7b\n",
            "google/codegemma-7b\n",
            "meta/codellama-70b\n",
            "microsoft/phi-3-medium-128k-instruct\n",
            "mistralai/codestral-22b-instruct-v0.1\n",
            "nvidia/llama3-chatqa-1.5-8b\n",
            "meta/llama-3.1-70b-instruct\n",
            "microsoft/phi-3-small-128k-instruct\n",
            "nvidia/nemotron-4-340b-instruct\n",
            "seallms/seallm-7b-v2.5\n",
            "microsoft/phi-3-medium-4k-instruct\n",
            "microsoft/phi-3-small-8k-instruct\n",
            "mistralai/mistral-large\n",
            "meta/llama3-70b-instruct\n",
            "meta/llama-3.1-405b-instruct\n",
            "microsoft/kosmos-2\n",
            "microsoft/phi-3-mini-128k-instruct\n",
            "mistralai/mamba-codestral-7b-v0.1\n",
            "writer/palmyra-med-70b-32k\n",
            "meta/llama3-8b-instruct\n",
            "aisingapore/sea-lion-7b-instruct\n",
            "liuhaotian/llava-v1.6-34b\n",
            "mistralai/mistral-7b-instruct-v0.2\n",
            "databricks/dbrx-instruct\n",
            "google/gemma-2-27b-it\n",
            "mistralai/mixtral-8x22b-instruct-v0.1\n",
            "meta/llama2-70b\n",
            "google/gemma-2-9b-it\n",
            "mediatek/breeze-7b-instruct\n",
            "google/gemma-2-2b-it\n",
            "mistralai/mixtral-8x7b-instruct-v0.1\n",
            "microsoft/phi-3-vision-128k-instruct\n",
            "google/gemma-7b\n",
            "nvidia/neva-22b\n",
            "nv-mistralai/mistral-nemo-12b-instruct\n",
            "meta/llama-3.1-8b-instruct\n",
            "adept/fuyu-8b\n",
            "google/paligemma\n",
            "nvidia/usdcode-llama3-70b-instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5XmeiiOWtuxC",
      "metadata": {
        "id": "5XmeiiOWtuxC"
      },
      "outputs": [],
      "source": [
        "\n",
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "for model in NVIDIAEmbeddings.get_available_models():\n",
        "  print(model.id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD0CIUiDh1dP",
        "outputId": "79b7726d-841c-4183-a78d-0369757d9a1e"
      },
      "id": "SD0CIUiDh1dP",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvidia/nv-embedqa-e5-v5\n",
            "nvidia/nv-embed-v1\n",
            "NV-Embed-QA\n",
            "snowflake/arctic-embed-l\n",
            "baai/bge-m3\n",
            "nvidia/nv-embedqa-mistral-7b-v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e37fe234-2bdb-4107-8483-efda9aa5e4ab",
      "metadata": {
        "id": "e37fe234-2bdb-4107-8483-efda9aa5e4ab"
      },
      "outputs": [],
      "source": [
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "# ChatNVIDIA.get_available_models()\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
        "\n",
        "instruct_llm_llama = ChatNVIDIA(model=\"meta/llama-3.1-70b-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LRx0XUf_Sdxw",
      "metadata": {
        "id": "LRx0XUf_Sdxw"
      },
      "source": [
        "### **Step 1**: Getting A Conversation\n",
        "\n",
        "Consider a conversation crafted using Llama-13B between a chat agent and an user named Eric. This dialogue, dense with details and potential diversions, provides a rich dataset for our study:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "8h3AgVADlS86"
      },
      "id": "8h3AgVADlS86",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_gen_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Generate a conversation between a Basura chat agent and an user named {user_name}\"\n",
        "    \"Basura is a waste management company in New York which caters its clients with theor waste production data\"\n",
        "    \"Basura just collects the data and do not handle waste produced in any way\"\n",
        "    \"Basura's employees manually collects data from clients properties daily\"\n",
        "    \"Make sure the topic revolves around: {topic}\"\n",
        "    \"Generate atleast 10 rows of conversation\"\n",
        "    \"Example conversation: [User] Hi, my name is Eric. I'd like to know about Basura\\n [Agent] Hi Eric, Basura ia an intelligent waste management company in New York....\"\n",
        "    \" Do not add Row numbers or additional information inside the tags [User] and [Agent]\"\n",
        ")\n",
        "\n",
        "conv_gen_chain = (conv_gen_prompt | instruct_llm_llama | StrOutputParser())\n",
        "\n",
        "conversation = conv_gen_chain.invoke({\"user_name\": \"Eric\", \"topic\": \"Waste management\"})\n",
        "pprint(conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "4SHsSS3GlUSR",
        "outputId": "193512a2-d510-46ae-c73f-c18d0fa46874"
      },
      "id": "4SHsSS3GlUSR",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mUser\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Hi, my name is Eric. I'd like to know about Basura.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mAgent\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Hi Eric, Basura is an intelligent waste management company in New York that provides its clients with \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mvaluable insights on their waste production data. We don't handle waste in any way, our main focus is on collecting\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mand analyzing data to help our clients make informed decisions.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mUser\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m That sounds interesting. How do you collect this data?\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mAgent\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Our team of trained professionals visits our clients' properties on a daily basis to collect the data. We \u001b[0m\n",
              "\u001b[1;38;2;118;185;0muse a combination of manual collection methods and IoT sensors to gather information on waste generation, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mcomposition, and disposal patterns.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mUser\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m I see. What kind of data can your clients expect to receive from you?\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mAgent\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Our clients receive regular reports on their waste production, including the types and quantities of waste \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mgenerated, recycling rates, and suggestions for improvement. This data can help them identify areas of inefficiency\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mand opportunities for cost savings.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mUser\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m That makes sense. How do you think data-driven waste management can benefit businesses and residents in New \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mYork?\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mAgent\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m By having access to accurate and timely data, our clients can optimize their waste management strategies, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mreduce their environmental footprint, and save money on waste disposal costs. Moreover, it can also help the city \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mas a whole to develop more effective waste management policies.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mUser\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m I'm a business owner myself, and I'm interested in learning more about how Basura can help my company. What \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mkind of support do you offer to your clients?\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mAgent\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m We offer personalized support to all our clients, including regular reporting, data analysis, and \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mrecommendations for improvement. Our team is always available to answer any questions or concerns you may have, and\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mwe also provide educational resources and workshops to help you get the most out of our services.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mUser\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m That sounds great. How can I get started with Basura?\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mAgent\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Simply fill out the contact form on our website, and one of our representatives will be in touch with you \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mto discuss your needs and provide a customized quote. We also offer a free trial period, so you can see the value \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mof our services for yourself before committing.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mUser\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Awesome, I'll definitely look into it. Thanks for the information!\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mAgent\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m You're welcome, Eric. It was my pleasure to help. If you have any more questions or need further \u001b[0m\n",
              "\u001b[1;38;2;118;185;0massistance, don't hesitate to reach out.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[User] Hi, my name is Eric. I'd like to know about Basura.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Agent] Hi Eric, Basura is an intelligent waste management company in New York that provides its clients with </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">valuable insights on their waste production data. We don't handle waste in any way, our main focus is on collecting</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and analyzing data to help our clients make informed decisions.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[User] That sounds interesting. How do you collect this data?</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Agent] Our team of trained professionals visits our clients' properties on a daily basis to collect the data. We </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">use a combination of manual collection methods and IoT sensors to gather information on waste generation, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">composition, and disposal patterns.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[User] I see. What kind of data can your clients expect to receive from you?</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Agent] Our clients receive regular reports on their waste production, including the types and quantities of waste </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generated, recycling rates, and suggestions for improvement. This data can help them identify areas of inefficiency</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and opportunities for cost savings.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[User] That makes sense. How do you think data-driven waste management can benefit businesses and residents in New </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">York?</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Agent] By having access to accurate and timely data, our clients can optimize their waste management strategies, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reduce their environmental footprint, and save money on waste disposal costs. Moreover, it can also help the city </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">as a whole to develop more effective waste management policies.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[User] I'm a business owner myself, and I'm interested in learning more about how Basura can help my company. What </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">kind of support do you offer to your clients?</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Agent] We offer personalized support to all our clients, including regular reporting, data analysis, and </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">recommendations for improvement. Our team is always available to answer any questions or concerns you may have, and</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">we also provide educational resources and workshops to help you get the most out of our services.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[User] That sounds great. How can I get started with Basura?</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Agent] Simply fill out the contact form on our website, and one of our representatives will be in touch with you </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to discuss your needs and provide a customized quote. We also offer a free trial period, so you can see the value </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of our services for yourself before committing.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[User] Awesome, I'll definitely look into it. Thanks for the information!</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Agent] You're welcome, Eric. It was my pleasure to help. If you have any more questions or need further </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">assistance, don't hesitate to reach out.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = conversation.split(\"\\n\\n\")\n",
        "conversation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJIkWaV4MKu1",
        "outputId": "2c4ed295-2d6c-40b4-dccc-a89722fb403c"
      },
      "id": "lJIkWaV4MKu1",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"[User] Hi, my name is Eric. I'd like to know about Basura.\",\n",
              " \"[Agent] Hi Eric, Basura is an intelligent waste management company in New York that provides its clients with valuable insights on their waste production data. We don't handle waste in any way, our main focus is on collecting and analyzing data to help our clients make informed decisions.\",\n",
              " '[User] That sounds interesting. How do you collect this data?',\n",
              " \"[Agent] Our team of trained professionals visits our clients' properties on a daily basis to collect the data. We use a combination of manual collection methods and IoT sensors to gather information on waste generation, composition, and disposal patterns.\",\n",
              " '[User] I see. What kind of data can your clients expect to receive from you?',\n",
              " '[Agent] Our clients receive regular reports on their waste production, including the types and quantities of waste generated, recycling rates, and suggestions for improvement. This data can help them identify areas of inefficiency and opportunities for cost savings.',\n",
              " '[User] That makes sense. How do you think data-driven waste management can benefit businesses and residents in New York?',\n",
              " '[Agent] By having access to accurate and timely data, our clients can optimize their waste management strategies, reduce their environmental footprint, and save money on waste disposal costs. Moreover, it can also help the city as a whole to develop more effective waste management policies.',\n",
              " \"[User] I'm a business owner myself, and I'm interested in learning more about how Basura can help my company. What kind of support do you offer to your clients?\",\n",
              " '[Agent] We offer personalized support to all our clients, including regular reporting, data analysis, and recommendations for improvement. Our team is always available to answer any questions or concerns you may have, and we also provide educational resources and workshops to help you get the most out of our services.',\n",
              " '[User] That sounds great. How can I get started with Basura?',\n",
              " '[Agent] Simply fill out the contact form on our website, and one of our representatives will be in touch with you to discuss your needs and provide a customized quote. We also offer a free trial period, so you can see the value of our services for yourself before committing.',\n",
              " \"[User] Awesome, I'll definitely look into it. Thanks for the information!\",\n",
              " \"[Agent] You're welcome, Eric. It was my pleasure to help. If you have any more questions or need further assistance, don't hesitate to reach out.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5hIp943mSqGZ",
      "metadata": {
        "id": "5hIp943mSqGZ"
      },
      "source": [
        "## Step 2: Constructing Our Vector Store Retriever\n",
        "\n",
        "To streamline similarity queries on our conversation, we can employ a vector store to help keep track of passages for us! **Vector Stores**, or vector storage systems, abstract away most of the low-level details of the embedding/comparison strategies and provide a simple interface to load and compare vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHMID2BUpjeP",
        "outputId": "e65d5815-089e-4dd1-fa2e-d26c340e2bec"
      },
      "id": "fHMID2BUpjeP",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.14)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.34)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.104)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.12 marshmallow-3.22.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1kE2-ejoTKKU",
      "metadata": {
        "id": "1kE2-ejoTKKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9b5c3e-26c2-4c71-d5a5-234f358f802b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 240 ms, sys: 9.1 ms, total: 249 ms\n",
            "Wall time: 3.31 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "## Streamlined from_texts FAISS vectorstore construction from text list\n",
        "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
        "retriever = convstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convstore.index.ntotal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiITfEZnx4jA",
        "outputId": "8d5a7f1e-cd19-45e0-fbe5-5b35f437e3ba"
      },
      "id": "EiITfEZnx4jA",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "muN66v5PW5dW",
      "metadata": {
        "id": "muN66v5PW5dW"
      },
      "source": [
        "The retriever can now be used like any other LangChain runnable to query the vector store for some relevant documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "kNZJTnlEWVYh",
      "metadata": {
        "id": "kNZJTnlEWVYh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "7cf2fe86-055c-4edb-a2a1-2e589c7ce9c9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Hi, my name is Eric. I'd like to know about Basura.\"\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I'm a business owner myself, and I'm interested in learning more about how Basura can \u001b[0m\n",
              "\u001b[32mhelp my company. What kind of support do you offer to your clients?\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I see. What kind of data can your clients expect to receive from you?'\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m You're welcome, Eric. It was my pleasure to help. If you have any more questions or \u001b[0m\n",
              "\u001b[32mneed further assistance, don't hesitate to reach out.\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User] Hi, my name is Eric. I'd like to know about Basura.\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User] I'm a business owner myself, and I'm interested in learning more about how Basura can </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">help my company. What kind of support do you offer to your clients?\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[User] I see. What kind of data can your clients expect to receive from you?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] You're welcome, Eric. It was my pleasure to help. If you have any more questions or </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">need further assistance, don't hesitate to reach out.\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(retriever.invoke(\"What is your name?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "SE1eDZTEWScC",
      "metadata": {
        "id": "SE1eDZTEWScC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "36447dc5-a88a-42b3-cbbc-ba5b272314df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m We offer personalized support to all our clients, including regular reporting, data \u001b[0m\n",
              "\u001b[32manalysis, and recommendations for improvement. Our team is always available to answer any questions or concerns you\u001b[0m\n",
              "\u001b[32mmay have, and we also provide educational resources and workshops to help you get the most out of our services.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Our clients receive regular reports on their waste production, including the types \u001b[0m\n",
              "\u001b[32mand quantities of waste generated, recycling rates, and suggestions for improvement. This data can help them \u001b[0m\n",
              "\u001b[32midentify areas of inefficiency and opportunities for cost savings.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m By having access to accurate and timely data, our clients can optimize their waste \u001b[0m\n",
              "\u001b[32mmanagement strategies, reduce their environmental footprint, and save money on waste disposal costs. Moreover, it \u001b[0m\n",
              "\u001b[32mcan also help the city as a whole to develop more effective waste management policies.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m That makes sense. How do you think data-driven waste management can benefit businesses\u001b[0m\n",
              "\u001b[32mand residents in New York?'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] We offer personalized support to all our clients, including regular reporting, data </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">analysis, and recommendations for improvement. Our team is always available to answer any questions or concerns you</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">may have, and we also provide educational resources and workshops to help you get the most out of our services.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Our clients receive regular reports on their waste production, including the types </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and quantities of waste generated, recycling rates, and suggestions for improvement. This data can help them </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">identify areas of inefficiency and opportunities for cost savings.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] By having access to accurate and timely data, our clients can optimize their waste </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">management strategies, reduce their environmental footprint, and save money on waste disposal costs. Moreover, it </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">can also help the city as a whole to develop more effective waste management policies.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[User] That makes sense. How do you think data-driven waste management can benefit businesses</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and residents in New York?'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(retriever.invoke(\"Where are the Rocky Mountains?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZEDEzpqmTYMv",
      "metadata": {
        "id": "ZEDEzpqmTYMv"
      },
      "source": [
        "## Step 3: Incorporating Conversation Retrieval Into Our Chain\n",
        "\n",
        "Now that we have our loaded retriever component as a chain, we can incorporate it into our existing chat system as before. Specifically, we can start with an ***always-on RAG formulation*** where:\n",
        "- **A retriever is always retrieving context by default**.\n",
        "- **A generator is acting on the retrieved context**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "64abe478-9bcb-4802-a26e-dc5a1756e313",
      "metadata": {
        "id": "64abe478-9bcb-4802-a26e-dc5a1756e313"
      },
      "outputs": [],
      "source": [
        "from langchain.document_transformers import LongContextReorder\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain.schema.runnable.passthrough import RunnableAssign\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "########################################################################\n",
        "## Utility Runnables/Methods\n",
        "def RPrint(preface=\"\"):\n",
        "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
        "    def print_and_return(x, preface):\n",
        "        if preface: print(preface, end=\"\")\n",
        "        pprint(x)\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "def docs2str(docs, title=\"Document\"):\n",
        "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
        "    out_str = \"\"\n",
        "    for doc in docs:\n",
        "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
        "        if doc_name:\n",
        "            out_str += f\"[Quote from {doc_name}] \"\n",
        "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
        "    return out_str\n",
        "\n",
        "## Optional; Reorders longer documents to center of output text\n",
        "long_reorder = RunnableLambda(LongContextReorder().transform_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "uue5UY3_TcvF",
      "metadata": {
        "id": "uue5UY3_TcvF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "c8aac7c2-6292-40d9-fcbe-18b3171645d7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mHello there! Basura operates as an intelligent waste management company and they are based in New York. They offer \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mvaluable insights on waste production data to their clients.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! Basura operates as an intelligent waste management company and they are based in New York. They offer </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">valuable insights on waste production data to their clients.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mBasura collects data through a combination of manual collection methods and the use of IoT sensors. Their team of \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtrained professionals visit clients' properties daily to gather information on waste generation, composition, and \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdisposal patterns.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Basura collects data through a combination of manual collection methods and the use of IoT sensors. Their team of </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">trained professionals visit clients' properties daily to gather information on waste generation, composition, and </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">disposal patterns.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "context_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Answer the question using only the context\"\n",
        "    \"\\n\\nRetrieved Context: {context}\"\n",
        "    \"\\n\\nUser Question: {question}\"\n",
        "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
        "        'question': (lambda x:x)\n",
        "    }\n",
        "    | context_prompt\n",
        "    # | RPrint()\n",
        "    | instruct_llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain_llama = (\n",
        "    {\n",
        "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
        "        'question': (lambda x:x)\n",
        "    }\n",
        "    | context_prompt\n",
        "    # | RPrint()\n",
        "    | instruct_llm_llama\n",
        "    | StrOutputParser()\n",
        ")\n",
        "pprint(chain.invoke(\"Where does Basura operate\"))\n",
        "print(\"========================================\")\n",
        "pprint(chain_llama.invoke(\"How basura collects data\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8wp9-8CbT0L9",
      "metadata": {
        "id": "8wp9-8CbT0L9"
      },
      "source": [
        "<br>\n",
        "\n",
        "Noticed some decent performance with this always-on retrieval node in the loop since the actual context being fed into the LLM remains relatively small. It's important to experiment with factors like embedding sizes, context limits, and model options to see what kinds of behavior you can expect and which efforts are worth taking to improve performance.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OnpOybOhUCTf",
      "metadata": {
        "id": "OnpOybOhUCTf"
      },
      "source": [
        "## 4: Automatic Conversation Storage\n",
        "\n",
        "Now that we see how our vector store memory unit should function, we can perform one last integration to allow our conversation to add new entries to our conversation: a runnable that calls the `add_texts` method for us to update the store state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "FsK6-AtRVdcZ",
      "metadata": {
        "id": "FsK6-AtRVdcZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "a51af921-bfc2-4cf6-b626-15a5794ccf22"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mThat doesn't seem to be related to our conversation. We were discussing how Basura can help support your business. \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mDid you have any further questions about our services?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">That doesn't seem to be related to our conversation. We were discussing how Basura can help support your business. </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Did you have any further questions about our services?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mIce cream, huh? I think I can take a pretty sweet guess\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m is ice cream your favorite food?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Ice cream, huh? I think I can take a pretty sweet guess</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> is ice cream your favorite food?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mHa ha, nice surprise! I was way off! I guessed ice cream because you mentioned earlier how excited you were about \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mgetting some ice cream, and I thought maybe that was a clue. But honey, huh? That's a unique favorite food! What do\u001b[0m\n",
              "\u001b[1;38;2;118;185;0myou love most about honey?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Ha ha, nice surprise! I was way off! I guessed ice cream because you mentioned earlier how excited you were about </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">getting some ice cream, and I thought maybe that was a clue. But honey, huh? That's a unique favorite food! What do</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">you love most about honey?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mYeah, I think I do! You told me your favorite food is actually honey, right? That was a sweet surprise, by the way!\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mI was way off with the ice cream guess.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Yeah, I think I do! You told me your favorite food is actually honey, right? That was a sweet surprise, by the way!</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">I was way off with the ice cream guess.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "\n",
        "########################################################################\n",
        "## Reset knowledge base and define what it means to add more messages.\n",
        "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
        "\n",
        "def save_memory_and_get_output(d, vstore):\n",
        "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
        "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
        "    return d.get('output')\n",
        "\n",
        "########################################################################\n",
        "\n",
        "# instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Answer the question using only the context. Do not hallucinate\"\n",
        "    \"\\n\\nRetrieved Context: {context}\"\n",
        "    \"\\n\\nUser Question: {input}\"\n",
        "    \"\\nAnswer the user conversationally. Make sure the conversation flows naturally.\\n\"\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "conv_chain = (\n",
        "    {\n",
        "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
        "        'input': (lambda x:x)\n",
        "    }\n",
        "    | RunnableAssign({'output' : chat_prompt | instruct_llm_llama | StrOutputParser()})\n",
        "    | partial(save_memory_and_get_output, vstore=convstore)\n",
        ")\n",
        "\n",
        "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n",
        "print()\n",
        "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
        "print()\n",
        "pprint(conv_chain.invoke(\"Actually, my favorite is honey! Not sure where you got that idea?\"))\n",
        "print()\n",
        "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convstore.index.ntotal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMBGqgs3yMQC",
        "outputId": "c62789f7-ba61-4f1f-fcfe-45e052e67803"
      },
      "id": "GMBGqgs3yMQC",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over all the chunks (documents) in the FAISS vectorstore\n",
        "for i, doc in enumerate(convstore.docstore._dict.values()):\n",
        "    print(f\"Chunk {i+1}: {doc.page_content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqTqwgX_yYlS",
        "outputId": "a10ca184-c75c-4f26-c99d-f108ab7b05ab"
      },
      "id": "rqTqwgX_yYlS",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1: [User] Hi, my name is Eric. I'd like to know about Basura.\n",
            "Chunk 2: [Agent] Hi Eric, Basura is an intelligent waste management company in New York that provides its clients with valuable insights on their waste production data. We don't handle waste in any way, our main focus is on collecting and analyzing data to help our clients make informed decisions.\n",
            "Chunk 3: [User] That sounds interesting. How do you collect this data?\n",
            "Chunk 4: [Agent] Our team of trained professionals visits our clients' properties on a daily basis to collect the data. We use a combination of manual collection methods and IoT sensors to gather information on waste generation, composition, and disposal patterns.\n",
            "Chunk 5: [User] I see. What kind of data can your clients expect to receive from you?\n",
            "Chunk 6: [Agent] Our clients receive regular reports on their waste production, including the types and quantities of waste generated, recycling rates, and suggestions for improvement. This data can help them identify areas of inefficiency and opportunities for cost savings.\n",
            "Chunk 7: [User] That makes sense. How do you think data-driven waste management can benefit businesses and residents in New York?\n",
            "Chunk 8: [Agent] By having access to accurate and timely data, our clients can optimize their waste management strategies, reduce their environmental footprint, and save money on waste disposal costs. Moreover, it can also help the city as a whole to develop more effective waste management policies.\n",
            "Chunk 9: [User] I'm a business owner myself, and I'm interested in learning more about how Basura can help my company. What kind of support do you offer to your clients?\n",
            "Chunk 10: [Agent] We offer personalized support to all our clients, including regular reporting, data analysis, and recommendations for improvement. Our team is always available to answer any questions or concerns you may have, and we also provide educational resources and workshops to help you get the most out of our services.\n",
            "Chunk 11: [User] That sounds great. How can I get started with Basura?\n",
            "Chunk 12: [Agent] Simply fill out the contact form on our website, and one of our representatives will be in touch with you to discuss your needs and provide a customized quote. We also offer a free trial period, so you can see the value of our services for yourself before committing.\n",
            "Chunk 13: [User] Awesome, I'll definitely look into it. Thanks for the information!\n",
            "Chunk 14: [Agent] You're welcome, Eric. It was my pleasure to help. If you have any more questions or need further assistance, don't hesitate to reach out.\n",
            "Chunk 15: User said I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\n",
            "Chunk 16: Agent said That doesn't seem to be related to our conversation. We were discussing how Basura can help support your business. Did you have any further questions about our services?\n",
            "Chunk 17: User said Can you guess what my favorite food is?\n",
            "Chunk 18: Agent said Ice cream, huh? I think I can take a pretty sweet guess... is ice cream your favorite food?\n",
            "Chunk 19: User said Actually, my favorite is honey! Not sure where you got that idea?\n",
            "Chunk 20: Agent said Ha ha, nice surprise! I was way off! I guessed ice cream because you mentioned earlier how excited you were about getting some ice cream, and I thought maybe that was a clue. But honey, huh? That's a unique favorite food! What do you love most about honey?\n",
            "Chunk 21: User said I see! Fair enough! Do you know my favorite food now?\n",
            "Chunk 22: Agent said Yeah, I think I do! You told me your favorite food is actually honey, right? That was a sweet surprise, by the way! I was way off with the ice cream guess.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KRMW6G7NVSWF",
      "metadata": {
        "id": "KRMW6G7NVSWF"
      },
      "source": [
        "Unlike the more automatic full-text or rule-based approaches to injecting context into the LLM, this approach ensures some amount of consolidation which can keep the context length from getting out of hand. It's still not a full-proof strategy on its own, but it's a stark improvement for unstructured conversations (and doesn't even require a strong instruction-tuned model to perform slot-filling)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9TPkh3SaLbqh",
      "metadata": {
        "id": "9TPkh3SaLbqh"
      },
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## Part 3: RAG For Document Chunk Retrieval\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jSjfCtiQnj9e",
      "metadata": {
        "id": "jSjfCtiQnj9e"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Loading And Chunking Your Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "Q7us66iPVc70",
      "metadata": {
        "id": "Q7us66iPVc70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf8787e-0435-41a2-e73e-60212c9a5695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructed Docstore with 22 chunks\n"
          ]
        }
      ],
      "source": [
        "from faiss import IndexFlatL2\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "print(f\"Constructed Docstore with {len(convstore.docstore._dict)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VU_VEx2mqJUK",
      "metadata": {
        "id": "VU_VEx2mqJUK"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Task 3: Implement Your RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "-RXSrb1GcNff",
      "metadata": {
        "id": "-RXSrb1GcNff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32625a4e-d48f-4268-db51-779d34f2f651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'd be happy to tell you about Basura. Basura is an intelligent waste management company located in New York. Rather than handling waste, our main focus is on collecting and analyzing data related to our clients' waste production. This enables us to provide valuable insights, which in turn helps companies make well-informed decisions regarding their waste management. If you're a business owner and interested in learning more, I can guide you on how to get started with Basura!"
          ]
        }
      ],
      "source": [
        "from langchain.document_transformers import LongContextReorder\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "import gradio as gr\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "# NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "# ChatNVIDIA.get_available_models()\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
        "# instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
        "\n",
        "#convstore = default_FAISS()\n",
        "\n",
        "def save_memory_and_get_output(d, vstore):\n",
        "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
        "    vstore.add_texts([\n",
        "        f\"User previously responded with {d.get('input')}\",\n",
        "        f\"Agent previously responded with {d.get('output')}\"\n",
        "    ])\n",
        "    return d.get('output')\n",
        "\n",
        "initial_msg = (\n",
        "    \"Hello! I am a Basura chat agent here to help the user!\"\n",
        "    \"\\n\\nHow can I help you?\"\n",
        ")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
        "    \"You are a Basura document chatbot. Help the user as they ask questions about documents.\"\n",
        "    \" User messaged just asked: {input}\\n\\n\"\n",
        "    \" From this, we have retrieved the following potentially-useful info: \"\n",
        "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
        "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Make your response conversational.)\"\n",
        "), ('user', '{input}')])\n",
        "\n",
        "stream_chain = chat_prompt | instruct_llm | StrOutputParser()\n",
        "\n",
        "\n",
        "retrieval_chain = (\n",
        "    {'input' : (lambda x: x)}\n",
        "    ## TODO: Make sure to retrieve history & context from convstore & docstore, respectively.\n",
        "    ## HINT: Our solution uses RunnableAssign, itemgetter, long_reorder, and docs2str\n",
        "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
        "    | RunnableAssign({'context' : itemgetter('input') | convstore.as_retriever()  | long_reorder | docs2str})\n",
        "    #| RPrint()\n",
        ")\n",
        "\n",
        "\n",
        "def chat_gen(message, history=[], return_buffer=True):\n",
        "    buffer = \"\"\n",
        "    ## First perform the retrieval based on the input message\n",
        "    retrieval = retrieval_chain.invoke(message)\n",
        "    line_buffer = \"\"\n",
        "\n",
        "    ## Then, stream the results of the stream_chain\n",
        "    for token in stream_chain.stream(retrieval):\n",
        "        buffer += token\n",
        "        ## If you're using standard print, keep line from getting too long\n",
        "        yield buffer if return_buffer else token\n",
        "\n",
        "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
        "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
        "\n",
        "\n",
        "## Start of Agent Event Loop\n",
        "test_question = \"Tell me about Basura!\"  ## <- modify as desired\n",
        "\n",
        "## Before you launch your gradio interface, make sure your thing works\n",
        "for response in chat_gen(test_question, return_buffer=False):\n",
        "    print(response, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9W7sC5Z6BfqM",
      "metadata": {
        "id": "9W7sC5Z6BfqM"
      },
      "source": [
        "### **Task 4:** Interact With Your Gradio Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "fMP3l7QL2JWT",
      "metadata": {
        "id": "fMP3l7QL2JWT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "d95083ce-d8eb-40cb-b695-38ded8040741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://0aca49fe66fb858838.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0aca49fe66fb858838.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://0aca49fe66fb858838.gradio.live\n",
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
        "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
        "\n",
        "try:\n",
        "    demo.launch(debug=True, share=True, show_api=False)\n",
        "    demo.close()\n",
        "except Exception as e:\n",
        "    demo.close()\n",
        "    print(e)\n",
        "    raise e"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}